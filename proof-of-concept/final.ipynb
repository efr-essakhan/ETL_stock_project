{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "87956abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Plan:\n",
    "# figure_out_Dates_needed > (extract) get_obj_of_date_needed_and_produce_single_df_from_them > (transform) do_transformations > store_into_target_s3_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7fea8",
   "metadata": {},
   "source": [
    "# Adapter layer: All functionality where we interact with our S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d67ff4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_df(bucket, key): # S3 > CSV > DF\n",
    "    csv_obj = bucket.Object(key=key).get().get('Body').read().decode('utf-8') #convert each filtered obj from bucket to a csv\n",
    "    data = StringIO(csv_obj) #convert the CSV object into string format into memory - so that it can be used without saving into hdd\n",
    "    df = pd.read_csv(data, delimiter=',') #create a panda's df out of the csv\n",
    "    return df\n",
    "\n",
    "def write_df_to_s3(bucket, df, key): # DF > .PARQUET > S3\n",
    "    out_buffer = BytesIO() #to handle binary data in memory\n",
    "    df.to_parquet(path=out_buffer, index=False) #Write a DataFrame as a parquet format (a file format) into BytesIO (memory)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key) #uploading the file\n",
    "    \n",
    "def write_df_to_s3_csv(bucket, df, key): # DF > .CSV > S3\n",
    "    out_buffer = StringIO() #to handle binary data in memory\n",
    "    df.to_csv(path_or_buf=out_buffer, index=False) #Write a DataFrame as a parquet format (a file format) into BytesIO (memory)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key) #uploading the file\n",
    "\n",
    "def list_files_in_prefix(src_bucket, prefix_date):\n",
    "    files = [obj.key for obj in src_bucket.objects.filter(Prefix=prefix_date)]\n",
    "    return files #files names in reality\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1a66750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Application layer - not core\n",
    "\n",
    "def return_date_list(src_bucket, arg_date, meta_key):\n",
    "    arg_date_minus1 = datetime.strptime(arg_date, '%Y-%m-%d').date() - timedelta(days=1) #Convert string into datetime obj and get previous date\n",
    "    today = datetime.today().date() \n",
    "    \n",
    "    try: #Catch exception if there is no meta file when read_csv_to_df method call\n",
    "        ##Handling Meta-File##\n",
    "        \n",
    "        df_meta = read_csv_to_df(src_bucket, meta_key)\n",
    "        #create a set out of a list of datetime's taken from the 'source_date' column of df_meta\n",
    "        meta_dates_set = set(pd.to_datetime(df_meta['source_date']).dt.date) \n",
    "\n",
    "    \n",
    "        all_dates_list = [(arg_date_minus1 + timedelta(days=x)) for x in range(0, (today-arg_date_minus1).days+1)] #Create list of string dates min_date to today\n",
    "\n",
    "        dates_not_in_meta = set(all_dates_list[1:]) - meta_dates_set #Remove any dates already in meta\n",
    "\n",
    "        if len(dates_not_in_meta) != 0:\n",
    "            min_date = min(set(all_dates_list[1:]) - meta_dates_set) - timedelta(days=1) #recalculated as now the src-dates have been removed (incase that removed the existing one).\n",
    "\n",
    "            #Now figure out the actual dates the report is needed for \n",
    "            reporting_dates = [date.strftime('%Y-%m-%d') for date in all_dates_list if date >= min_date]\n",
    "\n",
    "            #Need this inside one of the transform functions\n",
    "            report_min_date = (min_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        else:\n",
    "            #set these to later handle exceptions better\n",
    "            reporting_dates = [] #dates the report should be created for\n",
    "            report_min_date = datetime(2200, 1, 1).date() #indicates not needed\n",
    "            \n",
    "    except bucket.session.client('s3').exceptions.NoSuchKey:\n",
    "        ##If no Meta-File in existance##\n",
    "        reporting_dates = [(min_date + timedelta(days=x)).strftime('%Y-%m-%d') for x in range(0, (today-min_date).days+1)] #Create list of string dates min_date to today\n",
    "        #Need this inside one of the transform functions\n",
    "        report_min_date = arg_date\n",
    "       \n",
    "           \n",
    "    return report_min_date, reporting_dates\n",
    "\n",
    "\n",
    "#extract_date_list : dates to update (i.e. the dates that have been processed as a report and can go into meta_file)\n",
    "def update_meta_file(trg_bucket, meta_key, processed_date_list): \n",
    "    #read old meta_file as df > concatenate new df to meta_file df > writeback this df to s3\n",
    "    df_new = pd.DataFrame(columns=['source_date', 'datetime_of_processing'])\n",
    "    df_new['source_date'] = processed_date_list\n",
    "    df_new['datetime_of_processing'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    df_old = read_csv_to_df(trg_bucket, meta_key)\n",
    "    df_old = df_old.append(df_new, ignore_index=True)\n",
    "    write_df_to_s3_csv(trg_bucket, df_old, meta_key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856b91a",
   "metadata": {},
   "source": [
    "## Application Layer: The main purpose logic of our program i.e. doing ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bcba7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(src_bucket, date_list):\n",
    "    files = [key for date in date_list for key in list_files_in_prefix(src_bucket, date)] #nested list comprehension, left loop is first\n",
    "    df = pd.concat([read_csv_to_df(src_bucket, obj) for obj in files], ignore_index=True) #ignore_index: if there was any index, it will not be reset to a numerical one after append.\n",
    "    return df \n",
    "\n",
    "\n",
    "#Only creates the first report.\n",
    "def transform_report1(df, arg_date):\n",
    "    #Selecting the specific rows that we need for our wanted report\n",
    "    df = df.loc[:, ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']] #all rows needed to formulate report, but we want only specific columns\n",
    "    df.dropna(inplace=True) #drop any empty row (just to make sure - even though unlikely to be any)\n",
    "    \n",
    "    #Get opening price per ISIN and Day\n",
    "    #transform('first') is calling a group-by method. It returns the first non-NaN value in a series, or NaN if there is none on each row of the 'StartPrice' columns and through '.transform' it returns this columns - to be set as a new column.\n",
    "    df['opening_price'] = df.sort_values(by=['Time']).groupby(['ISIN','Date'])['StartPrice'].transform('first') #doesnt appear to have achieved anything\n",
    "    #transform('first'): It returns a series / dataframe with the same shape as the source group chunk, in which all values in every individual column are replaced with the first non-NaN value in this column, or with NaN if there is none.\n",
    "    \n",
    "    #Get closing price per ISIN and Day\n",
    "    df['closing_price'] = df.sort_values(by='Time').groupby(['ISIN', 'Date'])['StartPrice'].transform('last')\n",
    "    \n",
    "    #Aggregations to create most of the desired columns\n",
    "    df = df.groupby(['ISIN', 'Date'], as_index=False).agg(opening_price_eur=('opening_price', 'min'), closing_price_eur=('closing_price', 'min'), minimum_price_eur=('MinPrice', 'min'), maximum_price_eur=('MaxPrice', 'max'), daily_traded_volume=(\"TradedVolume\", 'sum'))\n",
    "    \n",
    "    #Create column that shows the percentage of change in previous days closing price to todays closing price.\n",
    "    df['prev_closing_price'] = df.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1) \n",
    "    df['change_prev_closing_%'] = (df['closing_price_eur'] - df['prev_closing_price']) / df['prev_closing_price'] * 100\n",
    "    df.drop(columns=['prev_closing_price'], inplace=True)\n",
    "    df = df.round(decimals=2)\n",
    "\n",
    "    #Filtering by the passed-in-argument data, as the report should not show older data.\n",
    "    df = df[df.Date >= arg_date]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load(trgt_bucket, df, trg_format, meta_key, processed_date_list):\n",
    "    ## Write to S3\n",
    "    key='stocks_daily_report_' + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + trg_format\n",
    "    write_df_to_s3(trgt_bucket, df, key)\n",
    "    update_meta_file(trgt_bucket, meta_key, processed_date_list)\n",
    "    return True\n",
    "    \n",
    "    \n",
    "############################################################################################\n",
    "def etl_report1(src_bucket, trgt_bucket, date_list, arg_date, trg_format, meta_key):\n",
    "    df = extract(src_bucket, date_list)\n",
    "    df = transform_report1(df, arg_date)\n",
    "    dates_that_will_be_processed = [date for date in date_list if date >= arg_date]\n",
    "    df = load(trgt_bucket, df, trg_format, meta_key, dates_that_will_be_processed)\n",
    "    return True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7222685d",
   "metadata": {},
   "source": [
    "## Main Function - program execution entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9d2e24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Parameters/Configurations\n",
    "    #TODO: Later read as config\n",
    "    arg_date = '2021-08-23' #This input-argument used to: All data will be extracted since this date.\n",
    "    src_bucket_name = 'deutsche-boerse-xetra-pds'\n",
    "    trg_bucket_name = 'stocks-etl-project-essa'\n",
    "    trg_format = '.parquet'\n",
    "    meta_key = 'meta_file.csv'\n",
    "    \n",
    "    #Init\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket_src = s3.Bucket(src_bucket_name)\n",
    "    bucket_trg = s3.Bucket(trg_bucket_name)\n",
    "    \n",
    "    # Run application\n",
    "    extract_date, date_list = return_date_list(bucket_trg, arg_date, meta_key) #returns all wanted object.keys\n",
    "    etl_report1(bucket_src, bucket_trg, date_list, extract_date, trg_format, meta_key)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fc2aa",
   "metadata": {},
   "source": [
    "## Run application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e69a360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0a30e",
   "metadata": {},
   "source": [
    "## Reading the uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "60bd9c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_file.csv\n",
      "stocks_daily_report_20210821_031514.parquet\n",
      "stocks_daily_report_20210822_013911.parquet\n",
      "stocks_daily_report_20210822_014404.parquet\n",
      "stocks_daily_report_20210822_143918.parquet\n",
      "stocks_daily_report_20210822_160116.parquet\n",
      "stocks_daily_report_20210823_161933.parquet\n",
      "stocks_daily_report_20210823_162635.parquet\n",
      "stocks_daily_report_20210823_164023.parquet\n",
      "stocks_daily_report_20210823_165830.parquet\n",
      "stocks_daily_report_20210824_033448.parquet\n",
      "stocks_daily_report_20210824_033552.parquet\n",
      "stocks_daily_report_20210824_035214.parquet\n"
     ]
    }
   ],
   "source": [
    "# #Parameters/Configurations\n",
    "# #TODO: Later read as config\n",
    "# arg_date = '2021-08-20' #This input-argument used to: All data will be extracted since this date.\n",
    "src_bucket_name = 'deutsche-boerse-xetra-pds'\n",
    "trg_bucket_name = 'stocks-etl-project-essa'\n",
    "# trg_format = '.parquet'\n",
    "\n",
    "#Init\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_src = s3.Bucket(src_bucket_name)\n",
    "bucket_trg = s3.Bucket(trg_bucket_name)\n",
    "\n",
    "\n",
    "for obj in bucket_trg.objects.all():\n",
    "    print(obj.key) #checking if the file exists in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dd3dc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prq_obj = bucket_trg.Object(key='stocks_daily_report_20210824_033552.parquet').get().get('Body').read()\n",
    "data = BytesIO(prq_obj) #pandas only accepts a file on disk or a file-like-object (BytesIO is a file-like-obj)\n",
    "df_report = pd.read_parquet(data) #using pandas built in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6d3c849e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Date</th>\n",
       "      <th>opening_price_eur</th>\n",
       "      <th>closing_price_eur</th>\n",
       "      <th>minimum_price_eur</th>\n",
       "      <th>maximum_price_eur</th>\n",
       "      <th>daily_traded_volume</th>\n",
       "      <th>change_prev_closing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT00000FACC2</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>8.44</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.43</td>\n",
       "      <td>8.68</td>\n",
       "      <td>522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT0000606306</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>20.50</td>\n",
       "      <td>20.60</td>\n",
       "      <td>20.50</td>\n",
       "      <td>20.60</td>\n",
       "      <td>135</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT0000609607</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>15.74</td>\n",
       "      <td>15.74</td>\n",
       "      <td>15.74</td>\n",
       "      <td>15.74</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT0000644505</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>109.60</td>\n",
       "      <td>111.60</td>\n",
       "      <td>109.60</td>\n",
       "      <td>111.60</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT0000652011</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>33.34</td>\n",
       "      <td>33.79</td>\n",
       "      <td>33.34</td>\n",
       "      <td>33.79</td>\n",
       "      <td>2371</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>XS2265368097</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>15.24</td>\n",
       "      <td>15.36</td>\n",
       "      <td>15.24</td>\n",
       "      <td>15.37</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>XS2265369574</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>19.89</td>\n",
       "      <td>20.21</td>\n",
       "      <td>19.89</td>\n",
       "      <td>20.21</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>XS2265369731</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>8.67</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.63</td>\n",
       "      <td>8.73</td>\n",
       "      <td>485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>XS2265370234</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>19.74</td>\n",
       "      <td>20.57</td>\n",
       "      <td>19.74</td>\n",
       "      <td>20.57</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>XS2284324667</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>27.79</td>\n",
       "      <td>28.44</td>\n",
       "      <td>27.79</td>\n",
       "      <td>28.48</td>\n",
       "      <td>5861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3056 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISIN        Date  opening_price_eur  closing_price_eur  \\\n",
       "0     AT00000FACC2  2021-08-23               8.44               8.68   \n",
       "1     AT0000606306  2021-08-23              20.50              20.60   \n",
       "2     AT0000609607  2021-08-23              15.74              15.74   \n",
       "3     AT0000644505  2021-08-23             109.60             111.60   \n",
       "4     AT0000652011  2021-08-23              33.34              33.79   \n",
       "...            ...         ...                ...                ...   \n",
       "3051  XS2265368097  2021-08-23              15.24              15.36   \n",
       "3052  XS2265369574  2021-08-23              19.89              20.21   \n",
       "3053  XS2265369731  2021-08-23               8.67               8.68   \n",
       "3054  XS2265370234  2021-08-23              19.74              20.57   \n",
       "3055  XS2284324667  2021-08-23              27.79              28.44   \n",
       "\n",
       "      minimum_price_eur  maximum_price_eur  daily_traded_volume  \\\n",
       "0                  8.43               8.68                  522   \n",
       "1                 20.50              20.60                  135   \n",
       "2                 15.74              15.74                    0   \n",
       "3                109.60             111.60                  190   \n",
       "4                 33.34              33.79                 2371   \n",
       "...                 ...                ...                  ...   \n",
       "3051              15.24              15.37                  900   \n",
       "3052              19.89              20.21                    0   \n",
       "3053               8.63               8.73                  485   \n",
       "3054              19.74              20.57                  300   \n",
       "3055              27.79              28.48                 5861   \n",
       "\n",
       "      change_prev_closing_%  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  \n",
       "...                     ...  \n",
       "3051                    NaN  \n",
       "3052                    NaN  \n",
       "3053                    NaN  \n",
       "3054                    NaN  \n",
       "3055                    NaN  \n",
       "\n",
       "[3056 rows x 8 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a317852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Date</th>\n",
       "      <th>opening_price_eur</th>\n",
       "      <th>closing_price_eur</th>\n",
       "      <th>minimum_price_eur</th>\n",
       "      <th>maximum_price_eur</th>\n",
       "      <th>daily_traded_volume</th>\n",
       "      <th>change_prev_closing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ISIN, Date, opening_price_eur, closing_price_eur, minimum_price_eur, maximum_price_eur, daily_traded_volume, change_prev_closing_%]\n",
       "Index: []"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report[df_report['ISIN'] == 'AT00000606306']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
